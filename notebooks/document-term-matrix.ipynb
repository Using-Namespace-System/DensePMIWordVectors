{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg \n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy.sparse import csr_array\n",
    "from scipy.sparse import find\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import zip_longest\n",
    "\n",
    "df = pd.read_csv('../input/abcnews-date-text.csv')\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "df['tokenized'] = df.headline_text.str.split(' ')\n",
    "\n",
    "df['length'] = df.tokenized.map(len)\n",
    "df = df.loc[df.length > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode and sanitize\n",
    "\n",
    "ex = df.explode('tokenized')\n",
    "\n",
    "ex = ex.loc[ex.tokenized.str.len() > 2]\n",
    "\n",
    "ex = ex.loc[~ex.tokenized.isin(stopwords_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ceate dictionary of words\n",
    "\n",
    "dictionary = ex.tokenized.drop_duplicates()\n",
    "\n",
    "dictionary = pd.Series(dictionary.tolist(), name='words')\n",
    "\n",
    "dictionary = dictionary.to_frame()\n",
    "\n",
    "dictionary_lookup = dictionary.to_dict()['words']\n",
    "\n",
    "dictionary['encode'] = dictionary.index + 1\n",
    "\n",
    "dictionary = dictionary.set_index('words')\n",
    "\n",
    "dictionary.encode = dictionary.encode\n",
    "dictionary = dictionary.to_dict()['encode']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encode = ex.tokenized.map(dictionary.get).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative\n",
    "#np.split(a[:, 1], np.unique(a[:, 0], return_index=True)[1][1:])\n",
    "\n",
    "encode.index.astype('int')\n",
    "encode.tokenized.astype('int')\n",
    "docs = encode.groupby(encode.index)['tokenized'].apply(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_docs = list(zip(*zip_longest(*docs.to_list(), fillvalue=0)))\n",
    "\n",
    "encoded_docs = csr_array(encoded_docs, dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "row_column_code = find(encoded_docs)\n",
    "\n",
    "word_sorted_index = row_column_code[2].argsort()\n",
    "\n",
    "doc_word = np.array([row_column_code[0][word_sorted_index], row_column_code[2][word_sorted_index]])\n",
    "\n",
    "doc_word_sorted_index = doc_word[0].argsort()\n",
    "\n",
    "doc_word = pd.DataFrame(np.array([doc_word[0][doc_word_sorted_index], doc_word[1][doc_word_sorted_index]]).T, columns=['doc','word'])\n",
    "\n",
    "doc_word.word = doc_word.word - 1\n",
    "\n",
    "doc_word_count  = doc_word.groupby(['doc','word']).size().to_frame('count').reset_index().to_numpy().T\n",
    "\n",
    "sparse_doc_word_matrix = csr_array((doc_word_count[2],(doc_word_count[0],doc_word_count[1])), shape=(np.size(encoded_docs, 0),len(dictionary)), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.06 TiB for an array with shape (1242915, 116795)\n",
    "#sparse_doc_word_matrix.toarray()\n",
    "\n",
    "documents, concepts, terms = svds(sparse_doc_word_matrix, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "terms_norm = PowerTransformer().fit_transform(terms)\n",
    "\n",
    "terms_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
