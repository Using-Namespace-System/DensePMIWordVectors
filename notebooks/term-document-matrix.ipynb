{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first in a series of posts on extracting word representations using statistical language modeling techniques. This first installment includes rudimentary corpus preprocessing, tokenization, vectorization, and inferences within the vector space model. The corpus is a public domain dataset of a million news headlines from the Australian Broadcasting Corporation between 2003 and 2021.\n",
    "\n",
    "All code blocks for this part of the project are included in this document. The first block includes the imports used in this part of the project.\n",
    "\n",
    "https://github.com/Using-Namespace-System/Syntagmatic-And-Paradigmatic-Word-Associations.git\n",
    "\n",
    "The Whole series can be cloned from the link above into a dev container and the configs will include the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "from matplotlib.pyplot import figure\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_array\n",
    "from scipy.sparse import find\n",
    "from pickleshare import PickleShareDB\n",
    "\n",
    "df = pd.read_csv('../input/abcnews-date-text.csv')\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the corpus is simplified to filtering out short headlines, small words, and stop-words. Each action is completed in pandas, I believe this may improve readability. The documents are exploded into a single series representing the whole corpus. From here stop-words can be filtered out. No further sanitation is performed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and sanitize\n",
    "\n",
    "#tokenize documents into individual words\n",
    "df['tokenized'] = df.headline_text.str.split(' ')\n",
    "\n",
    "#remove short documents from corpus\n",
    "df['length'] = df.tokenized.map(len)\n",
    "df = df.loc[df.length > 1]\n",
    "\n",
    "#use random subset of corpus\n",
    "df=df.sample(frac=0.50).reset_index(drop=True)\n",
    "\n",
    "#flatten all words into single series\n",
    "ex = df.explode('tokenized')\n",
    "\n",
    "#remove shorter words\n",
    "ex = ex.loc[ex.tokenized.str.len() > 2]\n",
    "\n",
    "#remove stop-words\n",
    "ex = ex.loc[~ex.tokenized.isin(stopwords_set)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of the corpus is performed by creating forward and backwards lookup dictionaries. Each unique word is represented as a unique number. This is a very simple method of tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of words\n",
    "\n",
    "#shuffle for sparse matrix visual\n",
    "dictionary = ex.tokenized.drop_duplicates().sample(frac=1)\n",
    "\n",
    "#dataframe with (index/code):word\n",
    "dictionary = pd.Series(dictionary.tolist(), name='words').to_frame()\n",
    "\n",
    "#store code:word dictionary for reverse encoding\n",
    "dictionary_lookup = dictionary.to_dict()['words']\n",
    "\n",
    "#offset index to prevent clash with zero fill\n",
    "dictionary['encode'] = dictionary.index + 1\n",
    "\n",
    "#store word:code dictionary for encoding\n",
    "dictionary = dictionary.set_index('words').to_dict()['encode']\n",
    "\n",
    "#use dictionary to encode each word to integer representation\n",
    "encode = ex.tokenized.map(dictionary.get).to_frame()\n",
    "encode.index.astype('int')\n",
    "encode.tokenized.astype('int')\n",
    "#un-flatten encoded words back into original documents\n",
    "docs = encode.tokenized.groupby(level=0).agg(tuple)\n",
    "\n",
    "#match up document indexes for reverse lookup\n",
    "df = df.sort_index().iloc[docs.index].reset_index(drop=True)\n",
    "docs = docs.reset_index()['tokenized']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In its simplest form the word vector for each term would be the one-hot(binary) encoding of the documents they are (1) and are not (0) present in. Likewise, the transform is comprised of document-word vectors where each is a one-hot encoding of the terms in the corpus that are and are not present in a document.\n",
    "\n",
    "In this instance the word vector is a count vector. This is similar to one-hot but is able to convey how many times the term occurred in the document.\n",
    "\n",
    "For the news headline dataset, document-wise term repetition is minimal and the statistical weight it provides is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#zero pad x dimension by longest sentence\n",
    "encoded_docs = list(zip(*zip_longest(*docs.to_list(), fillvalue=0)))\n",
    "\n",
    "#convert to sparse matrix\n",
    "encoded_docs = csr_array(encoded_docs, dtype=int)\n",
    "\n",
    "#convert to index for each word\n",
    "row_column_code = find(encoded_docs)\n",
    "\n",
    "#presort by words\n",
    "word_sorted_index = row_column_code[2].argsort()\n",
    "doc_word = np.array([row_column_code[0][word_sorted_index], row_column_code[2][word_sorted_index]])\n",
    "\n",
    "#presort by docs and words\n",
    "doc_word_sorted_index = doc_word[0].argsort()\n",
    "doc_word = pd.DataFrame(np.array([doc_word[0][doc_word_sorted_index], doc_word[1][doc_word_sorted_index]]).T, columns=['doc','word'])\n",
    "\n",
    "#offset code no longer needed after zero-fill\n",
    "doc_word.word = doc_word.word - 1\n",
    "\n",
    "#convert to index of word counts per document\n",
    "doc_word_count  = doc_word.groupby(['doc','word']).size().to_frame('count').reset_index().to_numpy().T\n",
    "\n",
    "#convert to sparse matrix\n",
    "sparse_word_doc_matrix = csr_array((doc_word_count[2],(doc_word_count[0],doc_word_count[1])), shape=(np.size(encoded_docs, 0),len(dictionary)), dtype=float).T\n",
    "\n",
    "#visualize sparse matrix\n",
    "fig = figure(figsize=(10,10))\n",
    "sparse_word_doc_matrix_visualization = fig.add_subplot(1,1,1)\n",
    "sparse_word_doc_matrix_visualization.spy(sparse_word_doc_matrix, markersize=0.007, aspect = 'auto')\n",
    "\n",
    "%store sparse_word_doc_matrix\n",
    "%store dictionary\n",
    "%store dictionary_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization below shows the words (y-axis) and the documents (x-axis) they are in. Across 600000 documents the terms in the corpus that re-occur more regularly form an interesting pattern of lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sparse Matrix](sparse_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that occur together often in the corpus also, as word vectors, are closer together in this 600000 dimensional vector space. This is demonstrated in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police</td>\n",
       "      <td>new</td>\n",
       "      <td>man</td>\n",
       "      <td>says</td>\n",
       "      <td>court</td>\n",
       "      <td>nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man</td>\n",
       "      <td>zealand</td>\n",
       "      <td>charged</td>\n",
       "      <td>govt</td>\n",
       "      <td>man</td>\n",
       "      <td>rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>investigate</td>\n",
       "      <td>laws</td>\n",
       "      <td>police</td>\n",
       "      <td>minister</td>\n",
       "      <td>accused</td>\n",
       "      <td>police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probe</td>\n",
       "      <td>police</td>\n",
       "      <td>court</td>\n",
       "      <td>australia</td>\n",
       "      <td>face</td>\n",
       "      <td>govt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>cases</td>\n",
       "      <td>murder</td>\n",
       "      <td>new</td>\n",
       "      <td>told</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>search</td>\n",
       "      <td>york</td>\n",
       "      <td>jailed</td>\n",
       "      <td>trump</td>\n",
       "      <td>faces</td>\n",
       "      <td>coast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2          3        4        5\n",
       "0       police      new      man       says    court      nsw\n",
       "1          man  zealand  charged       govt      man    rural\n",
       "2  investigate     laws   police   minister  accused   police\n",
       "3        probe   police    court  australia     face     govt\n",
       "4      missing    cases   murder        new     told  country\n",
       "5       search     york   jailed      trump    faces    coast"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#approximating cosine similarity with dot product of the term document matrix and its transform\n",
    "\n",
    "similarity_matrix  = sparse_word_doc_matrix @ sparse_word_doc_matrix.T\n",
    "\n",
    "#displaying slice of matrix with highest similarity scores\n",
    "\n",
    "similarity_matrix_compressed = similarity_matrix[(-similarity_matrix.sum(axis = 1)).argsort()[:6]].toarray()\n",
    "\n",
    "pd.DataFrame((-similarity_matrix_compressed).argsort(axis = 1)[:6,:6].T).applymap(dictionary_lookup.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents with the most similarities are closer together in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man bites police officer on new years day in a...</td>\n",
       "      <td>police search for man over police van crash</td>\n",
       "      <td>police search for man 'involved in police chas...</td>\n",
       "      <td>police union backs new police leadership team</td>\n",
       "      <td>new south wales government setting up new poli...</td>\n",
       "      <td>police investigate report of man impersonating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new half day public holidays on christmas eve ...</td>\n",
       "      <td>police search for man 'involved in police chas...</td>\n",
       "      <td>police search for man over police van crash</td>\n",
       "      <td>police union demands answers over appointment ...</td>\n",
       "      <td>new south wales records 1485 new cases of covi...</td>\n",
       "      <td>police charge man for impersonating officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>istanbul police arrest new years day gunman and</td>\n",
       "      <td>police search for police assault suspect</td>\n",
       "      <td>police search for man over fatal sydney stabbing</td>\n",
       "      <td>police union slams police commissioner for cho...</td>\n",
       "      <td>new south wales covid coronavirus five new loc...</td>\n",
       "      <td>queensland police describe man police believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>police make new years day drink driving arrests</td>\n",
       "      <td>police investigate report of man impersonating...</td>\n",
       "      <td>police investigate report of man impersonating...</td>\n",
       "      <td>nt police urge examination of new police numbers</td>\n",
       "      <td>new president for new south wales farmers</td>\n",
       "      <td>man charged with impersonating police officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new police information about tasmanian man mis...</td>\n",
       "      <td>queensland police describe man police believe</td>\n",
       "      <td>police to search sydney creek for missing man</td>\n",
       "      <td>wa police union president harry arnott stood a...</td>\n",
       "      <td>a new dairy body in new south wales has received</td>\n",
       "      <td>police search for man 'involved in police chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>brother of man fatally stabbed on new years da...</td>\n",
       "      <td>police investigation after man struck by polic...</td>\n",
       "      <td>queensland police describe man police believe</td>\n",
       "      <td>wa police commissioner backs down on new polic...</td>\n",
       "      <td>new south wales retains top ranking in new two...</td>\n",
       "      <td>man charge with impersonating police officer a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  man bites police officer on new years day in a...   \n",
       "1  new half day public holidays on christmas eve ...   \n",
       "2    istanbul police arrest new years day gunman and   \n",
       "3    police make new years day drink driving arrests   \n",
       "4  new police information about tasmanian man mis...   \n",
       "5  brother of man fatally stabbed on new years da...   \n",
       "\n",
       "                                                   1  \\\n",
       "0        police search for man over police van crash   \n",
       "1  police search for man 'involved in police chas...   \n",
       "2           police search for police assault suspect   \n",
       "3  police investigate report of man impersonating...   \n",
       "4      queensland police describe man police believe   \n",
       "5  police investigation after man struck by polic...   \n",
       "\n",
       "                                                   2  \\\n",
       "0  police search for man 'involved in police chas...   \n",
       "1        police search for man over police van crash   \n",
       "2   police search for man over fatal sydney stabbing   \n",
       "3  police investigate report of man impersonating...   \n",
       "4      police to search sydney creek for missing man   \n",
       "5      queensland police describe man police believe   \n",
       "\n",
       "                                                   3  \\\n",
       "0      police union backs new police leadership team   \n",
       "1  police union demands answers over appointment ...   \n",
       "2  police union slams police commissioner for cho...   \n",
       "3   nt police urge examination of new police numbers   \n",
       "4  wa police union president harry arnott stood a...   \n",
       "5  wa police commissioner backs down on new polic...   \n",
       "\n",
       "                                                   4  \\\n",
       "0  new south wales government setting up new poli...   \n",
       "1  new south wales records 1485 new cases of covi...   \n",
       "2  new south wales covid coronavirus five new loc...   \n",
       "3          new president for new south wales farmers   \n",
       "4   a new dairy body in new south wales has received   \n",
       "5  new south wales retains top ranking in new two...   \n",
       "\n",
       "                                                   5  \n",
       "0  police investigate report of man impersonating...  \n",
       "1        police charge man for impersonating officer  \n",
       "2      queensland police describe man police believe  \n",
       "3      man charged with impersonating police officer  \n",
       "4  police search for man 'involved in police chas...  \n",
       "5  man charge with impersonating police officer a...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#previewing document similarity\n",
    "\n",
    "doc_similarity_matrix  = sparse_word_doc_matrix.T @ sparse_word_doc_matrix\n",
    "\n",
    "#displaying slice of matrix with highest similarity scores\n",
    "\n",
    "doc_similarity_matrix_compressed = doc_similarity_matrix[(-doc_similarity_matrix.sum(axis = 1)).argsort()[:6]].toarray()\n",
    "\n",
    "pd.DataFrame((-doc_similarity_matrix_compressed).argsort(axis = 1)[:6,:6].T).applymap(df.headline_text.to_dict().get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
