{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Word Vectors from Decomposing a Word-Word Pointwise Mutual Information Matrix\nLets create some simple [word vectors](https://en.wikipedia.org/wiki/Word_embedding) by applying a [singular value decomposition](https://en.wikipedia.org/wiki/Singular-value_decomposition) to a [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) word-word matrix.  There are many other ways to create word vectors, but matrix decomposition is one of the most straightforward.  A well cited description of the technique used in this notebook can be found in Chris Moody's blog post [Stop Using word2vec](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/).    If you are interested in reading further about the history of word embeddings and a discussion of modern approaches check out the following blog post by Sebastian Ruder, [An overview of word embeddings and their connection to distributional semantic models](http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/).  Especially interesting to me is the work by Omar Levy, Yoav Goldberg, and Ido Dagan which shows that tuning hyperparameters is as (if not more) important as the algorithm chosen to build word vectors. [Improving Distributional Similarity with Lessons Learned from Word Embeddings](https://transacl.org/ojs/index.php/tacl/article/view/570).\n\nWe will be using the [\"A Million News Headlines\"](https://www.kaggle.com/therohk/million-headlines) dataset which contains headlines published over a period of 15 years from the Australian Broadcasting Corporation (ABC).\nIt is a great clean corpus that is large enough to be interesting and small enough to allow for quick calculations.  In this notebook tutorial we will implement as much as we can without using libraries that obfuscate the algorithm.  We're not going to write our own linear algebra or sparse matrix routines, but we will calculate unigram frequency, skipgram frequency, and the pointwise mutual information matrix \"by hand\".  Hopefully this will make the method easier to understand!   ","metadata":{"_cell_guid":"eee4b49d-b15a-446c-af88-9f308914f6f5","_uuid":"39bc9aa3b1dc1c834b1845ada4f253a1a8b6d7a6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport itertools\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom scipy.sparse import linalg \nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"_cell_guid":"ac9fffef-d727-4851-912b-ad80e958abcf","_uuid":"9cb0bce33b3e078eaf9bb593f265bf2ea13133b1","_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Data and Preview","metadata":{"_cell_guid":"ebb84c68-543c-4833-b9d0-5a76e3b5f674","_uuid":"b6ab8bbf571d5c9042d72b8be8fc286b1fc70005"}},{"cell_type":"code","source":"df = pd.read_csv('../input/abcnews-date-text.csv')\ndf.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Minimal Preprocessing\nWe're going to create a word-word co-occurrence matrix from the text in the headlines.  We will define two words as \"co-occurring\" if they appear in the same headline.  Using this definition, single word headlines are not interestintg for us.  Lets remove them as well as a common set of english stopwords.  ","metadata":{"_cell_guid":"471ed485-cb05-465b-9885-21d7449ed1b1","_uuid":"9cb0417c29d8d83c7be1e3798729f6754799d5ca"}},{"cell_type":"code","source":"headlines = df['headline_text'].tolist()\n# remove stopwords\nstopwords_set = set(stopwords.words('english'))\nheadlines = [\n    [tok for tok in headline.split() if tok not in stopwords_set] for headline in headlines\n]\n# remove single word headlines\nheadlines = [hl for hl in headlines if len(hl) > 1]\n# show results\nheadlines[0:20]","metadata":{"_cell_guid":"ee944644-9b88-4ae3-b546-6ef3445461ef","_uuid":"d5d57ad7adab679ad1aebb821c3b1a6d0c6044d7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unigrams\nNow lets calculate a unigram vocabulary.  The following code assigns a unique ID to each token, stores that mapping in two dictionaries (`tok2indx` and `indx2tok`), and counts how often each token appears in the corpus. ","metadata":{"_cell_guid":"bbb476d1-ba2c-4207-8621-4727ecf6a1c3","_uuid":"614a82366eda763f440f41a797c03559b41c7e5d"}},{"cell_type":"code","source":"unigram_counts = Counter()\nfor ii, headline in enumerate(headlines):\n    if ii % 200000 == 0:\n        print(f'finished {ii/len(headlines):.2%} of headlines')\n    for token in headline:\n        unigram_counts[token] += 1\n\ntok2indx = {tok: indx for indx, tok in enumerate(unigram_counts.keys())}\nindx2tok = {indx: tok for tok,indx in tok2indx.items()}\nprint('done')\nprint('vocabulary size: {}'.format(len(unigram_counts)))\nprint('most common: {}'.format(unigram_counts.most_common(10)))","metadata":{"_cell_guid":"c0c7a8b4-d82f-49ce-bf1c-62d02a9678c1","_uuid":"2d1cf9102cdf241bd367cb35d0ec7b741f1d4683","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Skipgrams\nNow lets calculate a skipgram vocabulary.  We will loop through each word in a headline (the focus word) and then form skipgrams by examing `back_window` words behind and `front_window` words in front of the focus word (the context words).  As an example, the first sentence (after preprocessing removes the stopword `against`) ,\n```\naba decides community broadcasting licence\n```\nwould produce the following skipgrams with `back_window`=`front_window`=`2`, \n```\n('aba', 'decides')\n('aba', 'community')\n('decides', 'aba')\n('decides', 'community')\n('decides', 'broadcasting')\n('community', 'aba')\n('community', 'decides')\n('community', 'broadcasting')\n('community', 'licence')\n('broadcasting', 'decides')\n('broadcasting', 'community')\n('broadcasting', 'licence')\n('licence', 'community')\n('licence', 'broadcasting')\n```","metadata":{"_cell_guid":"8a8bc838-6fa8-4f40-af0a-a11e962c33ed","_uuid":"d304e9ea8eb34b9b54a658347649d0c2cfac12d8"}},{"cell_type":"code","source":"# note add dynammic window hyperparameter\n\n# note we store the token vocab indices in the skipgram counter\n# note we use Levy, Goldberg, Dagan notation (word, context) as opposed to (focus, context)\nback_window = 2\nfront_window = 2\nskipgram_counts = Counter()\nfor iheadline, headline in enumerate(headlines):\n    tokens = [tok2indx[tok] for tok in headline]\n    for ii_word, word in enumerate(tokens):\n        ii_context_min = max(0, ii_word - back_window)\n        ii_context_max = min(len(headline) - 1, ii_word + front_window)\n        ii_contexts = [\n            ii for ii in range(ii_context_min, ii_context_max + 1) \n            if ii != ii_word]\n        for ii_context in ii_contexts:\n            skipgram = (tokens[ii_word], tokens[ii_context])\n            skipgram_counts[skipgram] += 1    \n    if iheadline % 200000 == 0:\n        print(f'finished {iheadline/len(headlines):.2%} of headlines')\n        \nprint('done')\nprint('number of skipgrams: {}'.format(len(skipgram_counts)))\nmost_common = [\n    (indx2tok[sg[0][0]], indx2tok[sg[0][1]], sg[1]) \n    for sg in skipgram_counts.most_common(10)]\nprint('most common: {}'.format(most_common))","metadata":{"_cell_guid":"a9565c4f-4de5-480d-87c8-7e0281b13ac4","scrolled":true,"_uuid":"aaf990fdd624c3cdc0f850ba203239b9e64c30d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sparse Matrices\n\nWe will calculate several matrices that store word-word information.  These matrices will be $N \\times N$ where $N \\approx 100,000$ is the size of our vocabulary.  We will need to use a sparse format so that it will fit into memory.  A nice implementation is available in [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).  To create these sparse matrices we create three iterables that store row indices, column indices, and data values. ","metadata":{"_cell_guid":"8bcfafa6-98f8-40b4-972e-d7df8c5c94f6","_uuid":"31537b3568b29f04549466e251aec617baa026f4"}},{"cell_type":"markdown","source":"# Word-Word Count Matrix\nOur very first word vectors will come from a word-word count matrix.  This matrix is symmetric so we can (equivalently) take the word vectors to be the rows or columns.  However we will try and code as if the rows are word vectors and the columns are context vectors. ","metadata":{"_cell_guid":"a4b24c6a-946a-40f8-937b-467a4f31770b","_uuid":"377457bac35cb7bdab01421e870049685d7cfbc5"}},{"cell_type":"code","source":"row_indxs = []\ncol_indxs = []\ndat_values = []\nii = 0\nfor (tok1, tok2), sg_count in skipgram_counts.items():\n    ii += 1\n    if ii % 1000000 == 0:\n        print(f'finished {ii/len(skipgram_counts):.2%} of skipgrams')    \n    row_indxs.append(tok1)\n    col_indxs.append(tok2)\n    dat_values.append(sg_count)\nwwcnt_mat = sparse.csr_matrix((dat_values, (row_indxs, col_indxs)))\nprint('done')","metadata":{"_cell_guid":"66089b1a-3fc8-4691-b083-cfa54eabc747","_uuid":"f3a107b7903f6ca9c6c8dc621b6ecb8e92cad51a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Similarity with Sparse Count Matrices","metadata":{"_cell_guid":"21f99f7e-4a19-4f85-afc9-e2c65ff70c4b","_uuid":"82317edef50fb1427ba2d8b6a84d2033bdb82a06"}},{"cell_type":"code","source":"def ww_sim(word, mat, topn=10):\n    \"\"\"Calculate topn most similar words to word\"\"\"\n    indx = tok2indx[word]\n    if isinstance(mat, sparse.csr_matrix):\n        v1 = mat.getrow(indx)\n    else:\n        v1 = mat[indx:indx+1, :]\n    sims = cosine_similarity(mat, v1).flatten()\n    sindxs = np.argsort(-sims)\n    sim_word_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n    return sim_word_scores\n    ","metadata":{"_cell_guid":"e4510fb3-03f9-4e74-b833-982e21011b60","_uuid":"60b7d344a2c923b6ac12a6ba10b84b42f4a199a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ww_sim('strike', wwcnt_mat)","metadata":{"_cell_guid":"6110a0c3-c020-4b7e-83d5-d4417f1a63ae","_uuid":"fa9079fc583ff3eeb941264025c7e2ac6508050e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalize each row using L2 norm\nwwcnt_norm_mat = normalize(wwcnt_mat, norm='l2', axis=1)","metadata":{"_cell_guid":"51431bda-a865-4345-b380-08f2b5b19ade","_uuid":"31fda68f475b96aca89edbb6af4d2b8842bdbe54","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# demonstrate normalization\nrow = wwcnt_mat.getrow(10).toarray().flatten()\nprint(np.sqrt((row*row).sum()))\n\nrow = wwcnt_norm_mat.getrow(10).toarray().flatten()\nprint(np.sqrt((row*row).sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ww_sim('strike', wwcnt_norm_mat)","metadata":{"_cell_guid":"8ad14fd9-0ff4-436c-897b-3af8cf41f446","_uuid":"70033cd3a5d70a951fa83656f11ad9ffc379e17c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pointwise Mutual Information Matrices\nThe pointwise mutual information (PMI) for a (word, context) pair in our corpus is defined as the probability of their co-occurrence divided by the probabilities of them appearing individually, \n$$\n{\\rm pmi}(w, c) = \\log \\frac{p(w, c)}{p(w) p(c)}\n$$\n\n$$\np(w, c) = \\frac{\nf_{i,j}\n}{\n\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n}, \\quad \np(w) = \\frac{\n\\sum_{j=1}^N f_{i,j}\n}{\n\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n}, \\quad\np(c) = \\frac{\n\\sum_{i=1}^N f_{i,j}\n}{\n\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n}\n$$\nwhere $f_{i,j}$ is the word-word count matrix we defined above.\nIn addition we can define the positive pointwise mutual information as, \n$$\n{\\rm ppmi}(w, c) = {\\rm max}\\left[{\\rm pmi(w,c)}, 0 \\right]\n$$\n\nNote that the definition of PMI above implies that ${\\rm pmi}(w, c) = {\\rm pmi}(c, w)$ and so this matrix will be symmetric.  However this is not true for the variant in which we smooth over the contexts.","metadata":{"_cell_guid":"56f1b0ac-ea0c-437e-b066-8918ba12251b","_uuid":"fd0988563bade6e57537a7eb980561be3db30461"}},{"cell_type":"code","source":"num_skipgrams = wwcnt_mat.sum()\nassert(sum(skipgram_counts.values())==num_skipgrams)\n\n# for creating sparce matrices\nrow_indxs = []\ncol_indxs = []\n\npmi_dat_values = []    # pointwise mutual information\nppmi_dat_values = []   # positive pointwise mutial information\nspmi_dat_values = []   # smoothed pointwise mutual information\nsppmi_dat_values = []  # smoothed positive pointwise mutual information\n\n# reusable quantities\n\n# sum_over_rows[ii] = sum_over_words[ii] = wwcnt_mat.getcol(ii).sum()\nsum_over_words = np.array(wwcnt_mat.sum(axis=0)).flatten()\n# sum_over_cols[ii] = sum_over_contexts[ii] = wwcnt_mat.getrow(ii).sum()\nsum_over_contexts = np.array(wwcnt_mat.sum(axis=1)).flatten()\n\n# smoothing\nalpha = 0.75\nsum_over_words_alpha = sum_over_words**alpha\nnca_denom = np.sum(sum_over_words_alpha)\n\n\n\nii = 0\nfor (tok_word, tok_context), sg_count in skipgram_counts.items():\n\n    ii += 1\n    if ii % 1000000 == 0:\n        print(f'finished {ii/len(skipgram_counts):.2%} of skipgrams')\n\n    # here we have the following correspondance with Levy, Goldberg, Dagan\n    #========================================================================\n    #   num_skipgrams = |D|\n    #   nwc = sg_count = #(w,c)\n    #   Pwc = nwc / num_skipgrams = #(w,c) / |D|\n    #   nw = sum_over_cols[tok_word]    = sum_over_contexts[tok_word] = #(w)\n    #   Pw = nw / num_skipgrams = #(w) / |D|\n    #   nc = sum_over_rows[tok_context] = sum_over_words[tok_context] = #(c)\n    #   Pc = nc / num_skipgrams = #(c) / |D|\n    #\n    #   nca = sum_over_rows[tok_context]^alpha = sum_over_words[tok_context]^alpha = #(c)^alpha\n    #   nca_denom = sum_{tok_content}( sum_over_words[tok_content]^alpha )\n    \n    nwc = sg_count\n    Pwc = nwc / num_skipgrams\n    nw = sum_over_contexts[tok_word]\n    Pw = nw / num_skipgrams\n    nc = sum_over_words[tok_context]\n    Pc = nc / num_skipgrams\n    \n    nca = sum_over_words_alpha[tok_context]\n    Pca = nca / nca_denom\n    \n    # note \n    # pmi = log {#(w,c) |D| / [#(w) #(c)]} \n    #     = log {nwc * num_skipgrams / [nw nc]}\n    #     = log {P(w,c) / [P(w) P(c)]} \n    #     = log {Pwc / [Pw Pc]}\n    pmi = np.log2(Pwc/(Pw*Pc))   \n    ppmi = max(pmi, 0)\n    spmi = np.log2(Pwc/(Pw*Pca))\n    sppmi = max(spmi, 0)\n    \n    row_indxs.append(tok_word)\n    col_indxs.append(tok_context)\n    pmi_dat_values.append(pmi)\n    ppmi_dat_values.append(ppmi)\n    spmi_dat_values.append(spmi)\n    sppmi_dat_values.append(sppmi)\n        \npmi_mat = sparse.csr_matrix((pmi_dat_values, (row_indxs, col_indxs)))\nppmi_mat = sparse.csr_matrix((ppmi_dat_values, (row_indxs, col_indxs)))\nspmi_mat = sparse.csr_matrix((spmi_dat_values, (row_indxs, col_indxs)))\nsppmi_mat = sparse.csr_matrix((sppmi_dat_values, (row_indxs, col_indxs)))\n\nprint('done')","metadata":{"_cell_guid":"7dbc6c77-7fd2-413b-b3ad-2006a5352725","_uuid":"c780eebf5597754c9a07b1a0e403986388edb39f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Similarity with Sparse PMI Matrices","metadata":{"_cell_guid":"739efb22-e1b0-4201-a0f3-0136fd1cf051","_uuid":"3712faead6d13739149977db8d590c7642f541d2"}},{"cell_type":"code","source":"ww_sim('strike', pmi_mat)","metadata":{"_cell_guid":"2a5e99fd-2457-42b1-a206-7812601144ab","_uuid":"179af697701291cad547e984d149cdcbcac18dd3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ww_sim('strike', ppmi_mat)","metadata":{"_cell_guid":"3dae981a-25ab-45eb-a39e-d38284165c12","_uuid":"a6d565e83c2fccbd0dc1e17a36feb2253fcdd248","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ww_sim('strike', spmi_mat)","metadata":{"_uuid":"ad93f771f8ceb60dfb3af926d902915372064379","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ww_sim('strike', sppmi_mat)","metadata":{"_uuid":"76cc04c466972013d3aa9ae10db11fad41575149","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Singular Value Decomposition\nWith the PMI and PPMI matrices in hand, we can apply a singular value decomposition to create dense word vectors from the sparse ones we've been using. ","metadata":{"_cell_guid":"dcfe9b79-e0cd-49e4-ac5a-b53482f202df","_uuid":"57d1fbb84a249feb9524012e03f4a8949bbdb1c9"}},{"cell_type":"code","source":"pmi_use = ppmi_mat\nembedding_size = 50\nuu, ss, vv = linalg.svds(pmi_use, embedding_size) ","metadata":{"_cell_guid":"b1bec58d-deda-4c45-89e6-6a2108ba8f56","_uuid":"2999b834c8b28f42d23b692ffcd6e1de99f4a5a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('vocab size: {}'.format(len(unigram_counts)))\nprint('embedding size: {}'.format(embedding_size))\nprint('uu.shape: {}'.format(uu.shape))\nprint('ss.shape: {}'.format(ss.shape))\nprint('vv.shape: {}'.format(vv.shape))","metadata":{"_cell_guid":"0e2291b9-5b66-4785-b9e1-fd2854ac9929","_uuid":"5cf87c0ad01129424deecb3c80a403155776bfd1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unorm = uu / np.sqrt(np.sum(uu*uu, axis=1, keepdims=True))\nvnorm = vv / np.sqrt(np.sum(vv*vv, axis=0, keepdims=True))\n#word_vecs = unorm\n#word_vecs = vnorm.T\nword_vecs = uu + vv.T\nword_vecs_norm = word_vecs / np.sqrt(np.sum(word_vecs*word_vecs, axis=1, keepdims=True))","metadata":{"_cell_guid":"0dcc5d06-e31c-46e2-97a5-7e7522fed8e5","_uuid":"444d98626f4cb22c5fe415532ead17fd5d5731aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_sim_report(word, sim_mat):\n    sim_word_scores = ww_sim(word, word_vecs)\n    for sim_word, sim_score in sim_word_scores:\n        print(sim_word, sim_score)\n        word_headlines = [hl for hl in headlines if sim_word in hl and word in hl][0:5]\n        for headline in word_headlines:\n            print(f'    {headline}')","metadata":{"_uuid":"cb591e15f276b3e7f5869802e69cb3ec3e5c6c13","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word = 'strike'\nword_sim_report(word, word_vecs)\n","metadata":{"_uuid":"85146587a2114cfb254c606a545b690e7fdf7574","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word = 'war'\nword_sim_report(word, word_vecs)","metadata":{"_cell_guid":"cdd49e45-fafc-4413-9a6d-baaff2bedf83","_uuid":"06c7a21dc517eaa85093e823b7c0e7c0e27672c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word = 'bank'\nword_sim_report(word, word_vecs)","metadata":{"_cell_guid":"e2a2cf1b-a4b9-4f68-b781-7dc47739a6eb","_uuid":"04f29beb799dffd9ef56cdc6bc995bc8bea45fce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word = 'car'\nword_sim_report(word, word_vecs)\n","metadata":{"_cell_guid":"6449138e-1b87-47a4-aeff-0ab35e1a2f19","_uuid":"6dc5606987d40b5d4789c8521811cecc0bfaf734","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word = 'football'\nword_sim_report(word, word_vecs)","metadata":{"_cell_guid":"953f82d5-bb76-4a1c-a8dc-e0ca6cd665ed","_uuid":"85c7d5d816687f3195b82ae5c2fad5c30048cff9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word = 'tech'\nword_sim_report(word, word_vecs)","metadata":{"_cell_guid":"a78e0cbb-f6b7-4b8b-af43-39e8fe2cf32d","_uuid":"56f4c9812a4e33f03c7eb529ca3a2081a3002dcb","trusted":true},"execution_count":null,"outputs":[]}]}