{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "from matplotlib.pyplot import figure\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_array\n",
    "from scipy.sparse import find\n",
    "from pickleshare import PickleShareDB\n",
    "import math\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "df = pd.read_csv('../input/abcnews-date-text.csv')\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and sanitize\n",
    "\n",
    "#tokenize documents into individual words\n",
    "df['tokenized'] = df.headline_text.str.split(' ')\n",
    "\n",
    "#remove short documents from corpus\n",
    "df['length'] = df.tokenized.map(len)\n",
    "df = df.loc[df.length > 1]\n",
    "\n",
    "#use random subset of corpus\n",
    "df=df.sample(frac=0.01).reset_index()\n",
    "\n",
    "#flatten all words into single series\n",
    "ex = df.explode('tokenized')\n",
    "\n",
    "#remove shorter words\n",
    "ex = ex.loc[ex.tokenized.str.len() > 2]\n",
    "\n",
    "#remove stopwords\n",
    "ex = ex.loc[~ex.tokenized.isin(stopwords_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ceate dictionary of words\n",
    "\n",
    "#shuffle for sparse matrix visual\n",
    "dictionary = ex.tokenized.drop_duplicates().sample(frac=1)\n",
    "\n",
    "#dataframe with (index/code):word\n",
    "dictionary = pd.Series(dictionary.tolist(), name='words').to_frame()\n",
    "\n",
    "#store code:word dictionary for reverse encoding\n",
    "dictionary_lookup = dictionary.to_dict()['words']\n",
    "\n",
    "#offset index to prevent clash with zero fill\n",
    "dictionary['encode'] = dictionary.index + 1\n",
    "\n",
    "#store word:code dictionary for encoding\n",
    "dictionary = dictionary.set_index('words').to_dict()['encode']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         6535\n",
       "0         5544\n",
       "0         6278\n",
       "0         7589\n",
       "0         1635\n",
       "         ...  \n",
       "12427    12916\n",
       "12428    11589\n",
       "12428     2993\n",
       "12428    11240\n",
       "12428     2013\n",
       "Name: tokenized, Length: 65850, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduced grouping from group by dataframeto to group by series and aggregated to tuple\n",
    "#improved from 30s to 20s with whole dataset\n",
    "\n",
    "#use dictionary to encode each word to integer representation\n",
    "encode = ex.tokenized.map(dictionary.get).to_frame()\n",
    "encode.index.astype('int')\n",
    "encode.tokenized.astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bidirectional_bigram  = encode.reset_index().rename(columns={'index':'doc1_1','tokenized':'word1'}).copy()\n",
    "\n",
    "bidirectional_bigram[['doc1_2','word2']] = bidirectional_bigram.drop(0).reset_index()[['doc1_1','word1']].astype(int)\n",
    "\n",
    "bidirectional_bigram = bidirectional_bigram.drop(bidirectional_bigram.tail(1).index).astype(int)\n",
    "\n",
    "#shift that produces bigrams results in invalid bigrams\n",
    "#number of invalid bigrams is equivalent to number of documents\n",
    "#remove bigrams not from the same document\n",
    "\n",
    "bidirectional_bigram = bidirectional_bigram[bidirectional_bigram.doc1_1 == bidirectional_bigram.doc1_2]\n",
    "\n",
    "bidirectional_bigram['bidirectional_hash'] = bidirectional_bigram.word1.pow(3) + bidirectional_bigram.word2.pow(3)\n",
    "\n",
    "bidirectional_hash_occurrences = bidirectional_bigram.bidirectional_hash.value_counts().to_dict()\n",
    "\n",
    "word_occurrences = encode.tokenized.value_counts().to_dict()\n",
    "\n",
    "num_bidirectional_bigrams = sum(bidirectional_hash_occurrences.values())\n",
    "\n",
    "num_words = sum(word_occurrences.values())\n",
    "\n",
    "#convert to symetric coordinates\n",
    "\n",
    "bidirectional_bigram = pd.concat([bidirectional_bigram, bidirectional_bigram.rename(columns={'word1':'word2','word2':'word1'})],ignore_index=True)\n",
    "\n",
    "bidirectional_bigram['bidirectional_hash_occurrences'] = bidirectional_bigram.bidirectional_hash.map(bidirectional_hash_occurrences.get)\n",
    "\n",
    "bidirectional_bigram['word1_occurrences'] = bidirectional_bigram.word1.map(word_occurrences.get)\n",
    "\n",
    "bidirectional_bigram['word2_occurrences'] = bidirectional_bigram.word2.map(word_occurrences.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "p(x,y): probability of term x,y co-occurrence over corpus is (number of term co-occurrences) / (number of all word co-occurrences)\n",
    "\n",
    "p(x): probability of term x individual occurrence over corpus is (number of term x occurrences) / (number of all word occurrences)\n",
    "\n",
    "p(y): probability of term y individual occurrence over corpus is (number of term y occurrences) / (number of all word occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_bigram['p_w1_w2'] = bidirectional_bigram.bidirectional_hash_occurrences / num_bidirectional_bigrams\n",
    "\n",
    "bidirectional_bigram['p_w1'] = bidirectional_bigram.word1_occurrences /num_words\n",
    "\n",
    "bidirectional_bigram['p_w2'] = bidirectional_bigram.word2_occurrences / num_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pointwise muutual information\n",
    "\n",
    "pmi(x;y) = log2((p(x,y))/(p(x)*p(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_bigram['pmi'] = np.log2(bidirectional_bigram.p_w1_w2/(bidirectional_bigram.p_w1*bidirectional_bigram.p_w2))\n",
    "\n",
    "standard_deviation = bidirectional_bigram.pmi.std()\n",
    "\n",
    "bidirectional_bigram.pmi = (bidirectional_bigram.pmi - bidirectional_bigram.pmi.mean())/standard_deviation\n",
    "\n",
    "#offsetting by one standard deviation to accomodate sparse zerofill and onefill on diagonal.\n",
    "\n",
    "bidirectional_bigram.pmi = (bidirectional_bigram.pmi - (bidirectional_bigram.pmi.min()-standard_deviation))/((bidirectional_bigram.pmi.max()+standard_deviation)-(bidirectional_bigram.pmi.min()-standard_deviation))\n",
    "\n",
    "bidirectional_bigram[['word1','word2']] = bidirectional_bigram[['word1','word2']] - 1\n",
    "\n",
    "pmi_matrix_coordinates = bidirectional_bigram[['word1','word2','pmi']].to_numpy().T\n",
    "\n",
    "sparse_pmi_matrix = csr_array((pmi_matrix_coordinates[2],(pmi_matrix_coordinates[0],pmi_matrix_coordinates[1])), shape=(np.size(encode, 0),len(dictionary)), dtype=float) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
